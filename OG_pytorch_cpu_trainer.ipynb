{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flush Experiments sqlite3 db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# from model_history import ModelHistory\n",
    "\n",
    "# history = ModelHistory('config.yaml')\n",
    "# history.clear_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def generate_synthetic_data(n_samples=1000, n_features=7, n_classes=5, test_size=0.2, random_state=42):\n",
    "    \"\"\"Generate synthetic data with well-separated classes.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate more samples initially to account for balancing\n",
    "    initial_samples = n_samples * 2\n",
    "    \n",
    "    # Generate base features with larger scale for better separation\n",
    "    X = np.random.randn(initial_samples, n_features) * 2\n",
    "    \n",
    "    # Create more distinct non-linear relationships\n",
    "    X[:, 0] = 3 * np.sin(X[:, 0]) + np.random.randn(initial_samples) * 0.1\n",
    "    X[:, 1] = 2 * np.exp(X[:, 1] / 3) + np.random.randn(initial_samples) * 0.1\n",
    "    X[:, 2] = X[:, 0] * X[:, 1] + np.random.randn(initial_samples) * 0.1\n",
    "    X[:, 3] = 2 * np.square(X[:, 3]) + np.random.randn(initial_samples) * 0.1\n",
    "    X[:, 4] = np.tanh(X[:, 4]) * 3 + np.random.randn(initial_samples) * 0.1\n",
    "    X[:, 5] = np.cos(X[:, 5]) * 2 + np.random.randn(initial_samples) * 0.1\n",
    "    X[:, 6] = np.sign(X[:, 6]) * np.log(np.abs(X[:, 6]) + 1) + np.random.randn(initial_samples) * 0.1\n",
    "    \n",
    "    # Generate target classes with more distinct decision boundaries\n",
    "    logits = np.zeros((initial_samples, n_classes))\n",
    "    logits[:, 0] = 2 * np.sin(X[:, 0]) + np.cos(X[:, 1]) + X[:, 4]\n",
    "    logits[:, 1] = X[:, 2] * X[:, 3] - np.square(X[:, 1]) + X[:, 5]\n",
    "    logits[:, 2] = np.exp(X[:, 0]/3) - np.sin(X[:, 2] * X[:, 3]) + X[:, 6]\n",
    "    logits[:, 3] = np.tanh(X[:, 4] + X[:, 5]) + np.cos(X[:, 0] * X[:, 1])\n",
    "    logits[:, 4] = np.sin(X[:, 6]) + np.exp(X[:, 2]/3) - np.cos(X[:, 3])\n",
    "    \n",
    "    # Add class-specific bias to balance classes\n",
    "    class_biases = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "    logits += class_biases[np.newaxis, :]\n",
    "    \n",
    "    # Convert to probabilities and select class\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    y = np.argmax(probs, axis=1)\n",
    "    \n",
    "    # Ensure we have enough samples per class after balancing\n",
    "    min_samples = n_samples // n_classes\n",
    "    balanced_indices = []\n",
    "    for class_idx in range(n_classes):\n",
    "        class_indices = np.where(y == class_idx)[0]\n",
    "        if len(class_indices) < min_samples:\n",
    "            # If we don't have enough samples, sample with replacement\n",
    "            class_indices = np.random.choice(class_indices, min_samples, replace=True)\n",
    "        else:\n",
    "            # If we htorch.__version__ave enough samples, sample without replacement\n",
    "            class_indices = np.random.choice(class_indices, min_samples, replace=False)\n",
    "        balanced_indices.extend(class_indices)\n",
    "    \n",
    "    # Ensure exact number of samples\n",
    "    if len(balanced_indices) > n_samples:\n",
    "        balanced_indices = np.random.choice(balanced_indices, n_samples, replace=False)\n",
    "    \n",
    "    X = X[balanced_indices]\n",
    "    y = y[balanced_indices]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    feature_cols = [f'feature_{i}' for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_cols)\n",
    "    df['target'] = y\n",
    "    \n",
    "    # Split with stratification\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        random_state=torch.__version__random_state,\n",
    "        stratify=df['target']\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def main():\n",
    "    # Create input_data directory if it doesn't exist\n",
    "    os.makedirs('input_data', exist_ok=True)\n",
    "    \n",
    "    # Generate data witorch.__version__th explicit sizes\n",
    "    train_df, val_df = generate_synthetic_data(\n",
    "        n_samples=1000,\n",
    "        n_features=7,  # Updated number of features\n",
    "        n_classes=5,   # Updated number of classes\n",
    "        test_size=0.2,torch.__version__\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Print detailed information\n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"Training data: {train_df.shape}\")\n",
    "    print(f\"Validation data: {val_df.shape}\")\n",
    "    print(\"\\nFeature names:\")\n",
    "    print(train_df.columtorch.__version__ns.tolist())\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_df['target'].value_counts(normalize=True).sort_index())\n",
    "    print(\"\\nClass distribution in validation set:\")\n",
    "    print(val_df['target'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # Save to CSV files\n",
    "    train_df.to_csv('input_data/train.csv', index=False)\n",
    "    val_df.to_csv('input_data/val.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "        \n",
    "# Set up logging\n",
    "def setup_logger(name='MLPTrainer'):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(f'{name}.log')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    \n",
    "    # Console handler\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, target_column):\n",
    "        self.features = torch.FloatTensor(df.drop(target_column, axis=1).values)\n",
    "        self.labels = torch.LongTensor(df[target_column].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes=3, dropout_rate=0.2, use_batch_norm=True):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "class PyTorchTrainer:\n",
    "    \"\"\"A generic PyTorch trainer class.\n",
    "    \n",
    "    Attributes:\n",
    "        model: PyTorch model to train\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        device: Device to train on (CPU/GPU)\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, device='cpu', verbose=False):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Trains the model for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # Save the best model and optimizer from hyperparameter tuning. Reload this best model after hyperparameter tuning. apply the reloaded model to the validation dataset as the final step, to compare its performance with the results of the train_final_model step.\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        return total_loss / len(train_loader), accuracy\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Evaluates the model on validation data.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        return total_loss / len(val_loader), accuracy, f1\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs, metric='accuracy'):\n",
    "        \"\"\"Trains the model for specified number of epochs. \n",
    "        Monitors specified validation metric for early stopping.\"\"\"\n",
    "        train_losses, val_losses = [], []\n",
    "        train_metrics, val_metrics = [], []\n",
    "        best_val_metric = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc='Training'):\n",
    "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            val_loss, val_accuracy, val_f1 = self.evaluate(val_loader)\n",
    "            \n",
    "            # Select metric based on config\n",
    "            train_metric = train_accuracy\n",
    "            val_metric = val_f1 if metric == 'f1' else val_accuracy\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_metrics.append(train_metric)\n",
    "            val_metrics.append(val_metric)\n",
    "            \n",
    "            best_val_metric = max(best_val_metric, val_metric)\n",
    "            \n",
    "            if self.verbose:\n",
    "                metric_name = 'F1' if metric == 'f1' else 'Accuracy'\n",
    "                metric_value = val_f1 if metric == 'f1' else val_accuracy\n",
    "                print(f'Epoch {epoch+1}/{epochs}: Val {metric_name}: {metric_value:.2f}%')\n",
    "        \n",
    "        self.plot_learning_curves(train_losses, val_losses, train_metrics, val_metrics, \n",
    "                                metric_name='F1-Score' if metric == 'f1' else 'Accuracy')\n",
    "        \n",
    "        return train_losses, val_losses, train_metrics, val_metrics, best_val_metric\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_learning_curves(train_losses, val_losses, train_metrics, val_metrics, metric_name='Accuracy'):\n",
    "        \"\"\"Plots the learning curves for loss and chosen metric (accuracy or F1).\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Normalize values for better visualization\n",
    "        max_loss = max(max(train_losses), max(val_losses))\n",
    "        max_metric = max(max(train_metrics), max(val_metrics))\n",
    "        \n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        sns.lineplot(data={\n",
    "            f\"Training {metric_name}\": [x/max_metric for x in train_metrics],\n",
    "            f\"Validation {metric_name}\": [x/max_metric for x in val_metrics],\n",
    "            \"Training Loss\": [x/max_loss for x in train_losses],\n",
    "            \"Validation Loss\": [x/max_loss for x in val_losses]\n",
    "        })\n",
    "        \n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Normalized Value\")\n",
    "        plt.title(f\"Training and Validation Loss and {metric_name} Curves\")\n",
    "        plt.legend()\n",
    "        plt.savefig('learning_curves.png')\n",
    "        plt.close()\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.best_trial_value = float('-inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_optimizer_state = None\n",
    "        self.best_params = None\n",
    "        os.makedirs(os.path.dirname(config['model']['save_path']), exist_ok=True)\n",
    "    \n",
    "    def save_best_model(self, model, optimizer, trial_value, params):\n",
    "        \"\"\"Save the best model and its metadata.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'optimizer_name': self.config['training']['optimizer_choice'],\n",
    "            'metric_name': self.config['training']['optimization_metric'],\n",
    "            'metric_value': trial_value,\n",
    "            'hyperparameters': params\n",
    "        }\n",
    "        torch.save(checkpoint, self.config['model']['save_path'])\n",
    "    \n",
    "    def create_model_and_optimizer(self, trial):\n",
    "        # Extract hyperparameters from trial\n",
    "        hidden_layers = []\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "        for i in range(n_layers):\n",
    "            hidden_layers.append(trial.suggest_int(f'hidden_layer_{i}', 32, 512))\n",
    "        \n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "        weight_decay = 0.0 if use_batch_norm else trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "        \n",
    "        # Create model\n",
    "        model = MLPClassifier(\n",
    "            input_size=self.config['model']['input_size'],\n",
    "            hidden_layers=hidden_layers,\n",
    "            num_classes=self.config['model']['num_classes'],\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        )\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = getattr(torch.optim, self.config['training']['optimizer_choice'])(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        trial_params = {\n",
    "            'n_layers': n_layers,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'lr': lr,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'use_batch_norm': use_batch_norm,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "        \n",
    "        return model, optimizer, trial_params\n",
    "    \n",
    "    def objective(self, trial, train_loader, val_loader):\n",
    "        model, optimizer, trial_params = self.create_model_and_optimizer(trial)\n",
    "        criterion = getattr(nn, self.config['training']['loss_function'])()\n",
    "        \n",
    "        trainer = PyTorchTrainer(\n",
    "            model, criterion, optimizer,\n",
    "            device=self.config['training']['device']\n",
    "        )\n",
    "        \n",
    "        patience = self.config['optimization']['early_stopping']['patience']\n",
    "        min_delta = self.config['optimization']['early_stopping']['min_delta']\n",
    "        best_metric = float('-inf')\n",
    "        patience_counter = 0\n",
    "        last_metric = float('-inf')\n",
    "        \n",
    "        # Add warm-up period\n",
    "        warm_up_epochs = 3\n",
    "        running_metrics = []\n",
    "        \n",
    "        for epoch in range(self.config['training']['epochs']):\n",
    "            trainer.train_epoch(train_loader)\n",
    "            _, accuracy, f1 = trainer.evaluate(val_loader)\n",
    "            \n",
    "            metric = f1 if self.config['training']['optimization_metric'] == 'f1' else accuracy\n",
    "            trial.report(metric, epoch)\n",
    "            \n",
    "            running_metrics.append(metric)\n",
    "            if len(running_metrics) > 3:\n",
    "                running_metrics.pop(0)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if metric > best_metric + min_delta:\n",
    "                best_metric = metric\n",
    "                patience_counter = 0\n",
    "                \n",
    "                if metric > self.best_trial_value:\n",
    "                    self.best_trial_value = metric\n",
    "                    self.save_best_model(model, optimizer, metric, trial_params)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Modified pruning logic with warm-up and relative threshold\n",
    "            if epoch >= warm_up_epochs:\n",
    "                avg_metric = sum(running_metrics) / len(running_metrics)\n",
    "                relative_deterioration = (best_metric - avg_metric) / (best_metric + 1e-8)\n",
    "                \n",
    "                if relative_deterioration > 0.3:  # 30% deterioration threshold\n",
    "                    raise optuna.TrialPruned(\"Trial pruned due to significant metric deterioration\")\n",
    "            \n",
    "            last_metric = metric\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        return best_metric\n",
    "    \n",
    "    def tune(self, train_loader, val_loader):\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            pruner=optuna.pruners.MedianPruner()\n",
    "        )\n",
    "        \n",
    "        study.optimize(\n",
    "            lambda trial: self.objective(trial, train_loader, val_loader),\n",
    "            n_trials=self.config['optimization']['n_trials']\n",
    "        )\n",
    "        \n",
    "        return study.best_trial, study.best_params\n",
    "\n",
    "def restore_best_model(config):\n",
    "    \"\"\"Utility function to restore the best model and its optimizer.\"\"\"\n",
    "    checkpoint = torch.load(config['model']['save_path'], weights_only=True)\n",
    "    \n",
    "    # Create model with saved hyperparameters\n",
    "    model = MLPClassifier(\n",
    "        input_size=config['model']['input_size'],\n",
    "        hidden_layers=checkpoint['hyperparameters']['hidden_layers'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        dropout_rate=checkpoint['hyperparameters']['dropout_rate'],\n",
    "        use_batch_norm=checkpoint['hyperparameters']['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = getattr(torch.optim, checkpoint['optimizer_name'])(\n",
    "        model.parameters(),\n",
    "        lr=checkpoint['hyperparameters']['lr'],\n",
    "        weight_decay=checkpoint['hyperparameters'].get('weight_decay', 0.0)\n",
    "    )\n",
    "    \n",
    "    # Load states\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'metric_name': checkpoint['metric_name'],\n",
    "        'metric_value': checkpoint['metric_value'],\n",
    "        'hyperparameters': checkpoint['hyperparameters']\n",
    "    }\n",
    "\n",
    "def save_best_params_to_config(config_path, best_trial, best_params):\n",
    "    \"\"\"Save best parameters to config file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create best_model section if it doesn't exist\n",
    "    if 'best_model' not in config:\n",
    "        config['best_model'] = {}\n",
    "    \n",
    "    # Format parameters for config\n",
    "    hidden_layers = [best_params[f'hidden_layer_{i}'] for i in range(best_params['n_layers'])]\n",
    "    \n",
    "    config['best_model'].update({\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'learning_rate': best_params['lr'],\n",
    "        'use_batch_norm': best_params['use_batch_norm'],\n",
    "        'weight_decay': best_params.get('weight_decay', 0.0),\n",
    "        'best_metric_name': config['training']['optimization_metric'],\n",
    "        'best_metric_value': best_trial.value\n",
    "    })\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "def train_final_model(config, train_loader, val_loader):\n",
    "    \"\"\"Train model using parameters from config.\"\"\"\n",
    "    best_model_config = config['best_model']\n",
    "    \n",
    "    final_model = MLPClassifier(\n",
    "        input_size=config['model']['input_size'],\n",
    "        hidden_layers=best_model_config['hidden_layers'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        dropout_rate=best_model_config['dropout_rate'],\n",
    "        use_batch_norm=best_model_config['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    criterion = getattr(nn, config['training']['loss_function'])()\n",
    "    optimizer = getattr(torch.optim, config['training']['optimizer_choice'])(\n",
    "        final_model.parameters(),\n",
    "        lr=best_model_config['learning_rate'],\n",
    "        weight_decay=best_model_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    final_trainer = PyTorchTrainer(\n",
    "        final_model, criterion, optimizer,\n",
    "        device=config['training']['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return final_trainer.train(\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        config['training']['epochs'],\n",
    "        metric=config['training']['optimization_metric']\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "class CPUOptimizer:\n",
    "    \"\"\"Handles CPU-specific optimizations for PyTorch training.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('CPUOptimizer')\n",
    "        self.cpu_info = cpuinfo.get_cpu_info()\n",
    "        \n",
    "    def detect_cpu_features(self):\n",
    "        \"\"\"Detect CPU features and capabilities.\"\"\"\n",
    "        features = {\n",
    "            'processor': self.cpu_info.get('brand_raw', 'Unknown'),\n",
    "            'architecture': self.cpu_info.get('arch', 'Unknown'),\n",
    "            'cores': psutil.cpu_count(logical=False),\n",
    "            'threads': psutil.cpu_count(logical=True),\n",
    "            'avx512': 'avx512' in self.cpu_info.get('flags', []),\n",
    "            'avx2': 'avx2' in self.cpu_info.get('flags', []),\n",
    "            'mkl': hasattr(torch, 'backends') and hasattr(torch.backends, 'mkl') and torch.backends.mkl.is_available(),\n",
    "            'ipex': hasattr(torch, 'xpu') or hasattr(torch, 'ipex')\n",
    "        }\n",
    "        return features\n",
    "        \n",
    "    def configure_optimizations(self):\n",
    "        \"\"\"Configure CPU-specific optimizations based on detected features.\"\"\"\n",
    "        features = self.detect_cpu_features()\n",
    "        optimizations = {}\n",
    "        \n",
    "        # Configure number of threads\n",
    "        if self.config['training']['cpu_optimization']['num_threads'] == 'auto':\n",
    "            optimizations['num_threads'] = features['threads']\n",
    "        else:\n",
    "            optimizations['num_threads'] = self.config['training']['cpu_optimization']['num_threads']\n",
    "        \n",
    "        # Configure MKL-DNN\n",
    "        optimizations['enable_mkldnn'] = (\n",
    "            features['avx512'] or features['avx2']\n",
    "        ) and self.config['training']['cpu_optimization']['enable_mkldnn']\n",
    "        \n",
    "        # Configure data types\n",
    "        optimizations['use_bfloat16'] = (\n",
    "            features['avx512'] and \n",
    "            self.config['training']['cpu_optimization']['use_bfloat16']\n",
    "        )\n",
    "        \n",
    "        # Set thread configurations\n",
    "        torch.set_num_threads(optimizations['num_threads'])\n",
    "        if hasattr(torch, 'set_num_interop_threads'):\n",
    "            torch.set_num_interop_threads(min(4, optimizations['num_threads']))\n",
    "        \n",
    "        # Enable MKL-DNN if available\n",
    "        if optimizations['enable_mkldnn']:\n",
    "            torch.backends.mkldnn.enabled = True\n",
    "        \n",
    "        self.log_optimization_config(features, optimizations)\n",
    "        return optimizations\n",
    "        \n",
    "    def log_optimization_config(self, features, optimizations):\n",
    "        \"\"\"Log CPU features and applied optimizations.\"\"\"\n",
    "        self.logger.info(\"CPU Configuration:\")\n",
    "        self.logger.info(f\"Processor: {features['processor']}\")\n",
    "        self.logger.info(f\"Architecture: {features['architecture']}\")\n",
    "        self.logger.info(f\"Physical cores: {features['cores']}\")\n",
    "        self.logger.info(f\"Logical threads: {features['threads']}\")\n",
    "        self.logger.info(\"\\nCPU Features:\")\n",
    "        self.logger.info(f\"AVX-512 support: {features['avx512']}\")\n",
    "        self.logger.info(f\"AVX2 support: {features['avx2']}\")\n",
    "        self.logger.info(f\"MKL support: {features['mkl']}\")\n",
    "        self.logger.info(f\"IPEX support: {features['ipex']}\")\n",
    "        self.logger.info(\"\\nApplied Optimizations:\")\n",
    "        self.logger.info(f\"Number of threads: {optimizations['num_threads']}\")\n",
    "        self.logger.info(f\"MKL-DNN enabled: {optimizations['enable_mkldnn']}\")\n",
    "        self.logger.info(f\"BFloat16 enabled: {optimizations['use_bfloat16']}\")\n",
    "\n",
    "def main():\n",
    "    config_path = 'config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Set up logging\n",
    "    setup_logger()\n",
    "    \n",
    "    # Initialize CPU optimization\n",
    "    cpu_optimizer = CPUOptimizer(config)\n",
    "    optimizations = cpu_optimizer.configure_optimizations()\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(config['training']['seed'])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_df = pd.read_csv(config['data']['train_path'])\n",
    "    val_df = pd.read_csv(config['data']['val_path'])\n",
    "    train_dataset = CustomDataset(train_df, config['data']['target_column'])\n",
    "    val_dataset = CustomDataset(val_df, config['data']['target_column'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # If best parameters don't exist in config, run hyperparameter tuning\n",
    "    if 'best_model' not in config:\n",
    "        tuner = HyperparameterTuner(config)\n",
    "        best_trial, best_params = tuner.tune(train_loader, val_loader)\n",
    "        save_best_params_to_config(config_path, best_trial, best_params)\n",
    "        # Reload config with saved parameters\n",
    "        config = load_config(config_path)\n",
    "    \n",
    "    print(\"\\nBest model parameters from config:\")\n",
    "    for key, value in config['best_model'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Restore best model from checkpoint\n",
    "    print(\"\\nRestoring best model from checkpoint...\")\n",
    "    restored = restore_best_model(config)\n",
    "    model = restored['model']\n",
    "    optimizer = restored['optimizer']\n",
    "    \n",
    "    # Create criterion for evaluation\n",
    "    criterion = getattr(nn, config['training']['loss_function'])()\n",
    "    \n",
    "    # Create trainer for evaluation\n",
    "    trainer = PyTorchTrainer(\n",
    "        model, criterion, optimizer,\n",
    "        device=config['training']['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate restored model\n",
    "    print(\"\\nEvaluating restored model on validation set...\")\n",
    "    val_loss, val_accuracy, val_f1 = trainer.evaluate(val_loader)\n",
    "    \n",
    "    metric_name = config['training']['optimization_metric']\n",
    "    metric_value = val_f1 if metric_name == 'f1' else val_accuracy\n",
    "    \n",
    "    print(f\"\\nRestored model performance:\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "    print(f\"\\nBest {metric_name.upper()} from tuning: {restored['metric_value']:.4f}\")\n",
    "    print(f\"Current {metric_name.upper()}: {metric_value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
